# NYC Taxi Data Pipeline - Delta Live Tables

A production-ready data pipeline for processing NYC Taxi trip data using Databricks Delta Live Tables (DLT) with Unity Catalog integration. The pipeline implements a medallion architecture (Bronze, Silver, Gold) with comprehensive data quality checks and business analytics.

## Overview

This project processes NYC taxi trip data through three layers:
- **Bronze Layer**: Raw data ingestion with basic quality checks
- **Silver Layer**: Cleaned and validated data ready for analytics
- **Gold Layer**: Business-ready aggregated tables for reporting and dashboards

**Key Features:**
- Medallion architecture with 16 tables/views
- Unity Catalog compatible
- Serverless compute optimized
- Comprehensive data quality expectations
- Modular, testable code structure
- CI/CD ready with Databricks Asset Bundles

## Architecture

### Medallion Data Flow

```
CSV Source
    ↓
Bronze Layer (4 tables)
    ├── bronze_taxi_raw (raw ingestion)
    ├── bronze_taxi_enriched (with computed fields)
    ├── bronze_taxi_streaming (streaming view)
    └── bronze_data_quality_metrics
    ↓
Silver Layer (5 tables)
    ├── silver_taxi_trips (cleaned trips)
    ├── silver_daily_trip_stats
    ├── silver_hourly_location_stats
    ├── silver_payment_analysis
    └── silver_data_quality
    ↓
Gold Layer (7 tables)
    ├── gold_daily_kpis
    ├── gold_peak_hours_analysis
    ├── gold_route_performance
    ├── gold_customer_segments
    ├── gold_weekly_trends
    ├── gold_revenue_breakdown
    └── gold_executive_summary
```

### Code Architecture

The pipeline uses a clean separation of concerns:
- **`dlt_pipeline.py`**: DLT orchestration (decorators, I/O, expectations)
- **`helpers/`**: Pure transformation functions (business logic)

This design enables:
- Easy unit testing of transformation logic
- Reusable transformation functions
- Clear separation between orchestration and business rules

## Project Structure

```
.
├── databricks.yml                  # Databricks Asset Bundle configuration
├── README.md
├── REFACTORING_GUIDE.md           # Detailed architecture documentation
│
├── src/
│   ├── dlt/
│   │   ├── dlt_pipeline.py        # Main DLT orchestrator
│   │   └── helpers/
│   │       ├── __init__.py
│   │       ├── bronze_transforms.py
│   │       ├── silver_transforms.py
│   │       └── gold_transforms.py
│   │
│   └── jobs/
│       └── bronze_ingestion.py    # Optional: Direct ingestion job
│
└── .github/
    └── workflows/
        └── deploy.yml             # CI/CD pipeline (optional)
```

## Prerequisites

- Databricks workspace with Unity Catalog enabled
- Databricks CLI installed and configured
- Python 3.8+
- Access to NYC Taxi dataset (available in Databricks sample datasets)

## Setup

### 1. Install Databricks CLI

```bash
# Install via pip
pip install databricks-cli

# Or via curl
curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
```

### 2. Configure Authentication

```bash
# Configure CLI with your workspace
databricks configure --token

# Enter your workspace URL and personal access token when prompted
```

### 3. Clone Repository

```bash
git clone <your-repo-url>
cd nyc-taxi-dlt-pipeline
```

### 4. Create Helper Package

```bash
# Ensure helpers is a Python package
touch src/dlt/helpers/__init__.py
```

## Configuration

### Environment Variables

The pipeline uses Databricks Asset Bundles for configuration. Key variables are defined in `databricks.yml`:

```yaml
variables:
  catalog_name: "dev_catalog"           # Unity Catalog name
  schema_name: "nyc_taxi"               # Schema name
  source_data_path: "/databricks-datasets/nyctaxi/..."
  notification_email: "your-email@company.com"
  dlt_edition: "ADVANCED"               # CORE, PRO, or ADVANCED
```

### Deployment Targets

Three deployment targets are configured:

- **dev**: Development environment with development mode enabled
- **staging**: Pre-production testing environment
- **prod**: Production environment with continuous mode

## Deployment

### Validate Bundle

```bash
databricks bundle validate -t dev
```

### Deploy to Environment

```bash
# Deploy to dev
databricks bundle deploy -t dev

# Deploy to staging
databricks bundle deploy -t staging

# Deploy to prod
databricks bundle deploy -t prod
```

### Run Pipeline

```bash
# Run DLT pipeline manually
databricks bundle run run_dlt_pipeline -t dev

# Check pipeline status
databricks pipelines get <pipeline-id>
```

### Scheduled Execution

The pipeline includes scheduled jobs configured in `databricks.yml`:
- **Bronze ingestion**: Daily at 1:00 AM UTC
- **DLT pipeline**: Daily at 2:00 AM UTC

Enable schedules by updating `pause_status: "UNPAUSED"` in the configuration.

## Data Quality

### Quality Expectations

The pipeline implements strict data quality checks:

**Bronze Layer:**
- Non-null vendor, pickup time, dropoff time
- Positive trip distance, passenger count, fare amount

**Silver Layer:**
- Trip distance between 0-100 miles
- Passenger count between 1-6
- Fare amount between 0-500 dollars
- Trip duration between 0-180 minutes
- Average speed between 0-80 mph
- Logical timestamp ordering

**Gold Layer:**
- Business rule validations
- Statistical outlier detection

### Quality Monitoring

Quality metrics tables track data health:
- `bronze_data_quality_metrics`: Bronze layer monitoring
- `silver_data_quality`: Bronze to silver pass-through rates

## Testing

### Unit Tests (Helper Functions)

```python
# Example: Test transformation logic
from helpers.bronze_transforms import enrich_with_computed_fields

def test_enrich_fields(spark_session):
    test_df = spark_session.createDataFrame([...])
    result = enrich_with_computed_fields(test_df)
    assert "trip_duration_minutes" in result.columns
```

### Integration Tests (Full Pipeline)

```bash
# Deploy to dev and run
databricks bundle deploy -t dev
databricks bundle run run_dlt_pipeline -t dev

# Verify tables created
databricks sql "SHOW TABLES IN dev_catalog.nyc_taxi"

# Check data quality
databricks sql "SELECT * FROM dev_catalog.nyc_taxi.bronze_data_quality_metrics"
```

## Monitoring

### Pipeline Monitoring

Monitor pipeline health through:
- Databricks DLT UI: Real-time pipeline execution
- Event logs: Detailed execution history
- Data quality metrics: Automated quality tracking

### Querying Tables

```sql
-- Check latest data
SELECT * FROM dev_catalog.nyc_taxi.gold_daily_kpis 
ORDER BY pickup_date DESC 
LIMIT 7;

-- Monitor data quality
SELECT 
    date,
    pass_rate_percentage,
    records_dropped
FROM dev_catalog.nyc_taxi.silver_data_quality
ORDER BY date DESC;
```

## CI/CD Integration

### GitHub Actions (Optional)

Create `.github/workflows/deploy.yml`:

```yaml
name: Deploy DLT Pipeline

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Install Databricks CLI
        run: curl -fsSL https://raw.githubusercontent.com/databricks/setup-cli/main/install.sh | sh
      - name: Deploy Bundle
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          databricks bundle validate -t prod
          databricks bundle deploy -t prod
```

### Required Secrets

Add to GitHub repository secrets:
- `DATABRICKS_HOST`: Your workspace URL
- `DATABRICKS_TOKEN`: Personal access token or service principal token

## Troubleshooting

### Common Issues

**Import Errors**
```bash
# Ensure helpers package exists
touch src/dlt/helpers/__init__.py
```

**Configuration Not Loading**
```yaml
# Verify configuration in databricks.yml
configuration:
  catalog_name: "${var.catalog_name}"
  schema_name: "${var.schema_name}"
```

**Pipeline Fails**
```bash
# Check DLT event logs
databricks pipelines get <pipeline-id>

# View detailed logs in Databricks UI
# Navigate to: Workflows > Delta Live Tables > [Your Pipeline] > Event log
```

### Debug Mode

Enable development mode for detailed logging:

```yaml
# databricks.yml
variables:
  dlt_development:
    default: "true"  # Enable for debugging
```

## Performance Optimization

The pipeline is optimized for performance:
- **Serverless compute**: Auto-scaling based on workload
- **Auto-optimize**: Automatic file compaction and indexing
- **Partitioning**: Tables partitioned by date for efficient queries
- **Change Data Feed**: Enabled for incremental processing

## Documentation

- **REFACTORING_GUIDE.md**: Detailed architecture and design decisions
- **Databricks DLT Docs**: https://docs.databricks.com/delta-live-tables/
- **Unity Catalog Docs**: https://docs.databricks.com/data-governance/unity-catalog/

## Contributing

1. Create a feature branch
2. Make changes and test locally
3. Deploy to dev environment for integration testing
4. Create pull request
5. After approval, deploy to staging and prod

## License

[Your License Here]

## Support

For issues or questions:
- Check troubleshooting section above
- Review DLT event logs in Databricks UI
- Contact data engineering team

## Changelog

### Version 1.0.0
- Initial release with medallion architecture
- 16 tables/views across Bronze, Silver, Gold layers
- Unity Catalog integration
- Comprehensive data quality checks
- Modular, testable code structure
# ============================================================================
# DATABRICKS ASSET BUNDLE - USER CONFIGURATION
# ============================================================================
# üéØ CUSTOMIZE THESE VALUES FOR YOUR PROJECT
# ============================================================================

bundle:
  name: test-deployment  # üìù CHANGE THIS: Your project name (e.g., "my-data-pipeline")

# ============================================================================
# üîß USER CONFIGURABLE VARIABLES - EDIT THESE FOR YOUR PROJECT
# ============================================================================
variables:
  # ----------------------------------------------------------------------------
  # PROJECT CONFIGURATION - Update these for your use case
  # ----------------------------------------------------------------------------
  project_name:
    description: "Project name for resources"
    default: "nyc_taxi"  # üìù CHANGE THIS: Your project name

  source_data_path:
    description: "Path to source data files"
    default: "/databricks-datasets/nyctaxi/tripdata/yellow/yellow_tripdata_2019-01.csv.gz"
    # üìù CHANGE THIS: Your data source path

  notification_email:
    description: "Email for pipeline notifications"
    default: "data-engineering@company.com"  # üìù CHANGE THIS: Your email

  # ----------------------------------------------------------------------------
  # UNITY CATALOG CONFIGURATION - Set your catalog/schema names
  # ----------------------------------------------------------------------------
  catalog_name:
    description: "Unity Catalog name (varies by environment)"
    default: "main"  # üìù CHANGE THIS: Your catalog name

  schema_name:
    description: "Schema name within the catalog"
    default: "nyc_taxi"  # üìù CHANGE THIS: Your schema name

  # ----------------------------------------------------------------------------
  # DEPLOYMENT TOGGLES - Control what gets deployed
  # ----------------------------------------------------------------------------
  deploy_dlt:
    description: "Deploy DLT pipeline (true/false)"
    default: "true"  # Set to "false" to skip DLT deployment

  deploy_workflow:
    description: "Deploy workflow jobs (true/false)"
    default: "true"  # Set to "false" to skip workflow deployment

  deploy_dlt_runner:
    description: "Deploy DLT runner job (true/false)"
    default: "true"  # Set to "false" to skip DLT runner deployment

  # ----------------------------------------------------------------------------
  # DLT PIPELINE CONFIGURATION
  # ----------------------------------------------------------------------------
  dlt_pipeline_name:
    description: "Name of the DLT pipeline"
    default: "NYC Taxi DLT Pipeline"  # üìù CHANGE THIS: Your pipeline name

  dlt_edition:
    description: "DLT edition: CORE, PRO, or ADVANCED"
    default: "ADVANCED"
    # CORE = Basic features, cheapest
    # PRO = Enhanced features
    # ADVANCED = All features including serverless, most expensive

  dlt_channel:
    description: "DLT release channel: CURRENT or PREVIEW"
    default: "CURRENT"  # Use PREVIEW for latest features (may be unstable)

  dlt_continuous:
    description: "Run DLT in continuous mode (always on)"
    default: "false"  # Set to "true" for real-time processing

  dlt_development:
    description: "Enable DLT development mode (faster iterations)"
    default: "true"  # Set to "false" for production

  # ----------------------------------------------------------------------------
  # WORKFLOW CONFIGURATION
  # ----------------------------------------------------------------------------
  workflow_name:
    description: "Name of the workflow job"
    default: "NYC Taxi Data Pipeline"  # üìù CHANGE THIS: Your workflow name

  workflow_schedule_enabled:
    description: "Enable workflow schedules"
    default: "false"  # Set to "true" to enable automatic scheduling

  workflow_cron_schedule:
    description: "Cron expression for workflow schedule"
    default: "0 0 1 * * ?"  # Daily at 1 AM UTC
    # Examples:
    # "0 0 1 * * ?" = Daily at 1 AM
    # "0 0 */6 * * ?" = Every 6 hours
    # "0 0 9 * * MON-FRI" = Weekdays at 9 AM

  # ----------------------------------------------------------------------------
  # COMPUTE CONFIGURATION
  # ----------------------------------------------------------------------------
  spark_version:
    description: "Databricks runtime version"
    default: "13.3.x-scala2.12"  # üìù CHANGE THIS: Latest stable version

  # ----------------------------------------------------------------------------
  # ENVIRONMENT SUFFIX - For feature branch deployments
  # ----------------------------------------------------------------------------
  environment_suffix:
    description: "Environment suffix for feature branches (e.g., '_feature_123')"
    default: ""

# ============================================================================
# ENVIRONMENT CONFIGURATIONS
# ============================================================================
# Each environment can override the variables above
# ============================================================================

environments:
  # ----------------------------------------------------------------------------
  # DEVELOPMENT ENVIRONMENT
  # ----------------------------------------------------------------------------
  dev:
    default: true
    workspace:
      root_path: /Workspace/Users/${workspace.current_user.userName}/.bundle/${bundle.name}${var.environment_suffix}

    variables:
      # Override catalog/schema for dev
      catalog_name: "dev_catalog"  # üìù CHANGE THIS: Your dev catalog
      schema_name: "${var.project_name}${var.environment_suffix}"

      # Dev-specific settings
      dlt_development: "true"
      dlt_continuous: "false"
      dlt_edition: "CORE"  # Use cheaper edition for dev
      workflow_schedule_enabled: "false"

  # ----------------------------------------------------------------------------
  # STAGING ENVIRONMENT
  # ----------------------------------------------------------------------------
  staging:
    workspace:
      root_path: /Workspace/Shared/.bundle/${bundle.name}/staging

    variables:
      # Override catalog/schema for staging
      catalog_name: "staging_catalog"  # üìù CHANGE THIS: Your staging catalog
      schema_name: "${var.project_name}"

      # Staging-specific settings
      dlt_development: "false"
      dlt_continuous: "false"
      dlt_edition: "ADVANCED"
      workflow_schedule_enabled: "true"

  # ----------------------------------------------------------------------------
  # PRODUCTION ENVIRONMENT
  # ----------------------------------------------------------------------------
  prod:
    workspace:
      root_path: /Workspace/Shared/.bundle/${bundle.name}/prod

    variables:
      # Override catalog/schema for prod
      catalog_name: "prod_catalog"  # üìù CHANGE THIS: Your prod catalog
      schema_name: "${var.project_name}"

      # Production-specific settings
      dlt_development: "false"
      dlt_continuous: "true"  # Always running in prod
      dlt_edition: "ADVANCED"
      workflow_schedule_enabled: "true"
      deploy_dlt_runner: "false"  # Use continuous mode instead

# ============================================================================
# RESOURCE DEFINITIONS
# ============================================================================
# These use the variables defined above - no hardcoded values!
# ============================================================================

resources:
  # ----------------------------------------------------------------------------
  # DLT PIPELINE - Uses serverless compute with Unity Catalog
  # ----------------------------------------------------------------------------
  pipelines:
    nyc_taxi_dlt_pipeline:
      # Conditional deployment based on deploy_dlt variable
      if: ${var.deploy_dlt} == "true"

      name: "${var.dlt_pipeline_name}${var.environment_suffix}"

      # Unity Catalog target - Uses variables, not hardcoded!
      target: "${var.catalog_name}.${var.schema_name}"

      # SERVERLESS - No cluster configuration needed
      serverless: true

      # Pipeline configuration - All values from variables
      configuration:
        source_path: "${var.source_data_path}"
        environment_suffix: "${var.environment_suffix}"
        pipeline_mode: "${{var.dlt_development == 'true' ? 'development' : 'production'}}"
        catalog_name: "${var.catalog_name}"
        schema_name: "${var.schema_name}"
        project_name: "${var.project_name}"

      # DLT pipeline code libraries
      # üìù UPDATE THESE PATHS: Point to your DLT files
      libraries:
        - file:
            path: ./src/dlt/nyc_taxi_dlt_bronze.py
        - file:
            path: ./src/dlt/nyc_taxi_dlt_silver.py
        - file:
            path: ./src/dlt/nyc_taxi_dlt_gold.py

      # Settings from variables
      development: ${var.dlt_development}
      channel: ${var.dlt_channel}
      edition: ${var.dlt_edition}
      continuous: ${var.dlt_continuous}
      photon: true
      catalog: ${var.catalog_name}

      # Notifications
      notifications:
        - email_recipients:
            - ${var.notification_email}
          alerts:
            - "on-update-failure"
            - "on-flow-failure"

  # ----------------------------------------------------------------------------
  # WORKFLOW JOBS - Uses serverless compute
  # ----------------------------------------------------------------------------
  jobs:
    # Legacy job - Your existing workflow
    nyc_taxi_pipeline:
      if: ${var.deploy_workflow} == "true"

      name: "${var.workflow_name}${var.environment_suffix}"

      tasks:
        - task_key: bronze_ingestion
          # üìù UPDATE THIS: Point to your job script
          spark_python_task:
            python_file: ./src/jobs/bronze_ingestion.py
            parameters:
              - "--catalog"
              - "${var.catalog_name}"
              - "--schema"
              - "${var.schema_name}"
              - "--source-path"
              - "${var.source_data_path}"

          timeout_seconds: 3600
          job_cluster_key: serverless_cluster

      # Serverless cluster definition
      job_clusters:
        - job_cluster_key: serverless_cluster
          new_cluster:
            spark_version: ${var.spark_version}
            data_security_mode: USER_ISOLATION
            runtime_engine: PHOTON
            spark_conf:
              spark.databricks.cluster.profile: serverless
              spark.databricks.unityCatalog.enabled: "true"
            custom_tags:
              environment: "${var.environment_suffix}"
              project: "${var.project_name}"
              cost_center: "data_engineering"

      # Schedule configuration
      schedule:
        quartz_cron_expression: "${var.workflow_cron_schedule}"
        timezone_id: "UTC"
        pause_status: "${{var.workflow_schedule_enabled == 'true' ? 'UNPAUSED' : 'PAUSED'}}"

      # Notifications
      email_notifications:
        on_failure:
          - ${var.notification_email}

    # DLT Runner Job - Triggers DLT pipeline
    run_dlt_pipeline:
      if: ${var.deploy_dlt} == "true" && ${var.deploy_dlt_runner} == "true"

      name: "DLT Pipeline Runner${var.environment_suffix}"

      tasks:
        - task_key: trigger_dlt_pipeline
          pipeline_task:
            pipeline_id: "${resources.pipelines.nyc_taxi_dlt_pipeline.id}"
            full_refresh: false

          timeout_seconds: 7200

      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
        timezone_id: "UTC"
        pause_status: "${{var.workflow_schedule_enabled == 'true' ? 'UNPAUSED' : 'PAUSED'}}"

      email_notifications:
        on_failure:
          - ${var.notification_email}
        on_success:
          - ${var.notification_email}

    # Combined workflow - DLT + downstream processing
    nyc_taxi_full_pipeline:
      if: ${var.deploy_dlt} == "true" && ${var.deploy_workflow} == "true"

      name: "Full Pipeline${var.environment_suffix}"

      tasks:
        # Task 1: Run DLT
        - task_key: run_dlt
          pipeline_task:
            pipeline_id: "${resources.pipelines.nyc_taxi_dlt_pipeline.id}"
            full_refresh: false
          timeout_seconds: 7200

#        # Task 2: Post-DLT processing
#        - task_key: post_dlt_processing
#          depends_on:
#            - task_key: run_dlt
#
#          spark_python_task:
#            # üìù UPDATE THIS: Point to your post-processing script
#            python_file: ./src/jobs/post_dlt_processing.py
#            parameters:
#              - "--catalog"
#              - "${var.catalog_name}"
#              - "--schema"
#              - "${var.schema_name}"
#
#          job_cluster_key: serverless_cluster
#          timeout_seconds: 3600

#        # Task 3: Data quality checks
#        - task_key: data_quality_checks
#          depends_on:
#            - task_key: post_dlt_processing
#
#          notebook_task:
#            # üìù UPDATE THIS: Point to your quality check notebook
#            notebook_path: ./src/notebooks/data_quality_checks
#            base_parameters:
#              catalog: ${var.catalog_name}
#              schema: ${var.schema_name}
#
#          job_cluster_key: serverless_cluster
#          timeout_seconds: 1800

      job_clusters:
        - job_cluster_key: serverless_cluster
          new_cluster:
            spark_version: ${var.spark_version}
            data_security_mode: USER_ISOLATION
            runtime_engine: PHOTON
            spark_conf:
              spark.databricks.cluster.profile: serverless
              spark.databricks.unityCatalog.enabled: "true"

      schedule:
        quartz_cron_expression: "0 0 3 * * ?"
        timezone_id: "UTC"
        pause_status: "${{var.workflow_schedule_enabled == 'true' ? 'UNPAUSED' : 'PAUSED'}}"

      email_notifications:
        on_failure:
          - ${var.notification_email}

  # ----------------------------------------------------------------------------
  # PERMISSIONS (Optional)
  # ----------------------------------------------------------------------------
  permissions:
    pipeline_nyc_taxi_dlt:
      if: ${var.deploy_dlt} == "true"
      pipeline_id: "${resources.pipelines.nyc_taxi_dlt_pipeline.id}"
      access_control:
        - group_name: "data_engineers"  # üìù CHANGE THIS: Your group name
          permission_level: "CAN_MANAGE"
        - group_name: "data_analysts"  # üìù CHANGE THIS: Your group name
          permission_level: "CAN_VIEW"

    job_nyc_taxi_pipeline:
      if: ${var.deploy_workflow} == "true"
      job_id: "${resources.jobs.nyc_taxi_pipeline.id}"
      access_control:
        - group_name: "data_engineers"
          permission_level: "CAN_MANAGE"
        - group_name: "data_analysts"
          permission_level: "CAN_VIEW"